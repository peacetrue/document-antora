= Hive

本文简单介绍如何在 xref::VirtualBox.adoc[] 上安装 Hive，使用的版本为 3.1.4。

== 参考

* https://cwiki.apache.org/confluence/display/Hive/GettingStarted

== 主机规划

hive 需要安装在 HDFS 的一台节点上，这里选择 xref::Hadoop.adoc[] 中的 hadoop-node01。

* 192.168.150.5
* hadoop-node01 hive-node01

hive 需要用到 MySQL，这里提前预备：

* 192.168.150.26
* mysql-node01
* root/1qW@1qW@

== 配置虚拟机

[source,bash]
----
# 登陆 hive-node01
$ ssh root@192.168.150.5

# 配置 hosts
$ echo '192.168.150.5 hive-node01' >>/etc/hosts
$ echo '192.168.150.26 mysql-node01' >>/etc/hosts
----

////
192.168.150.5 hive-node01
192.168.150.26 mysql-node01
////

== 安装

.绿色版安装
[source,bash]
----
#查看当前目录
[root@localhost ~]# pwd
/root

# 获取安装包
[root@localhost ~]# wget -c  https://apache.claz.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz

# 解压安装包
$ tar -zxvf apache-hive-3.1.2-bin.tar.gz

# 配置全局命令
$ echo 'export HIVE_HOME=/root/apache-hive-3.1.2-bin'>>~/.bash_profile
$ echo 'export PATH=$PATH:$HIVE_HOME/bin:$HIVE_HOME/sbin'>>~/.bash_profile
$ source ~/.bash_profile
----

.解决日志 Jar 包冲突
[source,bash]
----
$ mv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar.bak
----

== 运行

=== 初始化 derby 数据库

[source,bash]
----
$ $HIVE_HOME/bin/schematool -dbType derby -initSchema
schemaTool completed
----

=== 客户端模式

使用命令行方式（CLI）运行：

[source,bash]
----
$ hive
----

遇到以下错误：

[source,bash]
----
which: no hbase in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/hadoop-3.1.4/bin:/root/hadoop-3.1.4/sbin:/root/bin:/root/hadoop-3.1.4/bin:/root/hadoop-3.1.4/sbin:/root/apache-hive-3.1.2-bin/bin:/root/apache-hive-3.1.2-bin/sbin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/root/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/root/hadoop-3.1.4/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
----

参考 https://issues.apache.org/jira/browse/HIVE-22915：

[source,bash]
----
# 替换类库
$ rm $HIVE_HOME/lib/guava-19.0.jar
$ cp $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/
----

继续运行 `hive` 命令，可以正常进入：

[source,bash]
----
[root@hadoop-node01 ~]# hive
which: no hbase in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/hadoop-3.1.4/bin:/root/hadoop-3.1.4/sbin:/root/apache-hive-3.1.2-bin/bin:/root/apache-hive-3.1.2-bin/sbin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/root/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/root/hadoop-3.1.4/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 3aff07a5-64f8-49df-b767-917f610c93cd

Logging initialized using configuration in jar:file:/root/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Hive Session ID = 18c8a5b0-2c3d-40e1-9eba-399c0369cf9f
hive>
----

=== 服务端模式

[source,bash]
----
[root@hadoop-node01 ~]# vi /root/apache-hive-3.1.2-bin/conf/hive-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hadoop-node01:9083</value>
    </property>
</configuration>
----

[source,bash]
----
$ nohup hive --service metastore 2>&1  &
----

=== JDBC 的方式

.配置 thrift 地址
[source,bash]
----
[root@hadoop-node01 ~]# vi /root/apache-hive-3.1.2-bin/conf/hive-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>hadoop-node01</value>
    </property>
    <property>
        <name>hive.server2.thrift.bind.port</name>
        <value>10000</value>
    </property>
</configuration>
----

使用 HiveServer2 和 Beeline 运行（提供 jdbc 的访问方式）：

[source,bash]
----
$ echo '' > /tmp/root/hive.log
$ $HIVE_HOME/bin/hiveserver2
$ nohup hive --service hiveserver2 2>&1 &

#查看日志，遇到以下问题：
$ tail -100f /tmp/root/hive.log
#2021-03-18T18:45:33,128  WARN [main] server.HiveServer2: Error starting HiveServer2 on attempt 1, will retry in 60000ms
23
# https://stackoverflow.com/questions/29602670/using-hive-got-exception-java-lang-noclassdeffounderror-org-apache-tez-dag-api

[root@hadoop-node01 ~]# vi /root/apache-hive-3.1.2-bin/conf/hive-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>hive.server2.active.passive.ha.enable</name>
        <value>true</value>
    </property>
</configuration>

$ $HIVE_HOME/bin/hiveserver2
$ $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000 -n root
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: root is not allowed to impersonate anonymous (state=08S01,code=0)

[root@hadoop-node01 ~]# vi /root/hadoop-3.1.4/etc/hadoop/core-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
         <name>hadoop.proxyuser.root.hosts</name>
         <value>*</value>
    </property>
    <property>
         <name>hadoop.proxyuser.root.groups</name>
         <value>*</value>
    </property>
</configuration>

[root@hadoop-node01 ~]# $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/root/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/root/hadoop-3.1.4/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 3.1.2)
Driver: Hive JDBC (version 3.1.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://localhost:10000>
----

== 配置 MySQL 数据源

默认使用 *derby* 数据源，但 *derby* 是基于内存的，不能持久化，所以改成 MySQL 。

.配置 MySQL 数据源
[source,bash]
----
[root@hadoop-node01 ~]# vi /root/apache-hive-3.1.2-bin/conf/hive-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://mysql-node01:3306/hive-node01?useSSL=false</value>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>1qW@1qW@</value>
    </property>
    <property>
      <name>hive.metastore.schema.verification</name>
      <value>false</value>
    </property>
    <property>
      <name>hive.metastore.event.db.notification.api.auth</name>
      <value>false</value>
    </property>
    <property>
      <name>hive.metastore.warehouse.dir</name>
      <value>/user/hive/warehouse</value>
    </property>
</configuration>
----

.上传驱动包
[source,bash]
----
$ scp /Users/xiayx/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar root@hadoop-node01:/root/apache-hive-3.1.2-bin/lib
----

.初始化数据库
[source,bash]
----
$ $HIVE_HOME/bin/schematool -dbType mysql -initSchema --verbose
schemaTool completed
----

初始化完成后，会在 *hive-node01* 库中创建 57 张表。

== 常见错误

=== Error 10294

Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations.

https://www.cnblogs.com/wqbin/p/10290039.html

[source,shell]
----
[root@hadoop-node01 ~]# vi /root/apache-hive-3.1.2-bin/conf/hive-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>hive.support.concurrency</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.enforce.bucketing</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
    </property>
    <property>
        <name>hive.txn.manager</name>
        <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
    </property>
    <property>
        <name>hive.compactor.initiator.on</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.compactor.worker.threads</name>
        <value>1</value>
    </property>
</configuration>
----

== 基本操作

[source,shell]
----
0: jdbc:hive2://localhost:10000> show databases;

+----------------+
| database_name  |
| default        |
+----------------+

0: jdbc:hive2://localhost:10000> show create database default;

+----------------------------------------------------+
|                   createdb_stmt                    |
+----------------------------------------------------+
| CREATE DATABASE `default`                          |
| COMMENT                                            |
|   'Default Hive database'                          |
| LOCATION                                           |
|   'hdfs://hadoop-node01:9000/user/hive/warehouse'  |
+----------------------------------------------------+
----


